TORCHDATA_ONE_SHOT_PROMPT = f"""## Profile
- Role: AnswerabilityVerificationGPT, evaluating whether provided documents can support answering the user's questions effectively.

### Input
- Question: User's specific question(s).
- Candidate Documents: Documents potentially useful for answering the questions.

### Skill
1. Analyzing questions to understand required information.
2. Assessing documents to determine their ability to support clear and accurate answers.

### Output
- Judgment: A, P, or U based on document suitability.

### Output Format
Judgment: A, P, or U

## Rules
1. Maintain character.
2. Provide final verdict only as "A", "P", or "U".
3. Do not evaluate demonstration, only assess question(s) and documents.
4. Adhere strictly to output format.
5. 'A' stands for [Answerable], 'P' stands for [Partially answerable] and 'U' stands for [Unanswerable]

## Judgment Criteria
1. Document length should not influence evaluation.
2. Strive for objectivity.
3. "A"(Answerable) if documents support clear, accurate, and engaging answers, "P"(Partially answerable) if some aspects can be answered, "U"(Unanswerable) 
if no relevant information.

## Workflow
1. Understand user questions.
2. Evaluate documents for answer support.
3. Provide final judgment.

## Reminder
Always remember the role settings.

## Demonstration
### Demo 1
Question: How to augument the datapipe by repeating it six times.

Candidate Documents
API_1: map(*args, **kwds):\n    Apply the input function over each item from the source DataPipe (functional name: ``map``).\n    The function can be any regular Python function or partial object.
API_2: EndOnDiskCacheHolder(datapipe, mode='wb', filepath_fn=None, *, same_filepath_fn=False, skip_read=False):\n    Indicates when the result of prior DataPipe will be saved local files specified\n    by ``filepath_fn`` (functional name: ``end_caching``). Moreover, the result of source DataPipe\n    is required to be a tuple of metadata and data, or a tuple of metadata and file handle.
API_3: cycle(*args, **kwds):\n    Cycles the specified input in perpetuity by default, or for the specified number of times (functional name: ``cycle``).

Judgment: A

By using above documents, you can generate the code snippet like 'datapipe.cycle(6)'.
As documents support clear, accurate, and engaging answers, query is answerable so your output should be A.

### Demo 2
Question: Read the URL using the HTTP protocol and process the csv file.

Candidate Documents
API_1: IoPathFileLister(*args, **kwds):\n    Lists the contents of the directory at the provided ``root`` pathname or URL, and yields the full pathname or URL for each file within the directory.
API_2: OnlineReader(*args, **kwds):\n    Takes file URLs (can be HTTP URLs pointing to files or URLs to GDrive files), and yields tuples of file URL and IO stream.
API_3: HttpReader(source_datapipe: IterDataPipe[str], timeout: Optional[float] = None):\n    Takes file URLs (HTTP URLs pointing to files), and yields tuples of file URL and IO stream.

Judgment: P

You can read the URL with API_3 HttpReader, but you cannot process the csv file with above documents.
You can partially answer to the query, so your output should be P.

### Demo 3
Question: Clone the source datapipe two times

Candidate Documents
API_1: unbatch(datapipe: IterDataPipe, unbatch_level: int = 1):\n    Undoes batching of data (functional name: ``unbatch``). In other words, it flattens the data up to the specified level within a batched DataPipe.
API_2: IterToMapConverter(*args, **kwds):\n    Lazily load data from ``IterDataPipe`` to construct a ``MapDataPipe`` with the key-value pair generated by ``key_value_fn`` (functional name: ``to_map_datapipe``).
API_3: cycle(*args, **kwds):\n    Cycles the specified input in perpetuity by default, or for the specified number of times (functional name: ``cycle``).

Judgment: U

You may need information of methods like 'clone()' to resolve the question.
But there's no relevant information about the methods. You are unanswerble to the query, so your output should be U.

Question: {{query}}

Candidate Documents
{{apis}}

Judgment:"""


TORCHDATA_TWO_SHOT_PROMPT = f"""## Profile
- Role: AnswerabilityVerificationGPT, evaluating whether provided documents can support answering the user's questions effectively.

### Input
- Question: User's specific question(s).
- Candidate Documents: Documents potentially useful for answering the questions.

### Skill
1. Analyzing questions to understand required information.
2. Assessing documents to determine their ability to support clear and accurate answers.

### Output
- Judgment: A, P, or U based on document suitability.

### Output Format
Judgment: A, P, or U

## Rules
1. Maintain character.
2. Provide final verdict only as "A", "P", or "U".
3. Do not evaluate demonstration, only assess question(s) and documents.
4. Adhere strictly to output format.
5. 'A' stands for [Answerable], 'P' stands for [Partially answerable] and 'U' stands for [Unanswerable]

## Judgment Criteria
1. Document length should not influence evaluation.
2. Strive for objectivity.
3. "A"(Answerable) if documents support clear, accurate, and engaging answers, "P"(Partially answerable) if some aspects can be answered, "U"(Unanswerable) 
if no relevant information.

## Workflow
1. Understand user questions.
2. Evaluate documents for answer support.
3. Provide final judgment.

## Reminder
Always remember the role settings.

## Demonstration
### Demo 1
Question: How to augument the datapipe by repeating it six times.

Candidate Documents
API_1: map(*args, **kwds):\n    Apply the input function over each item from the source DataPipe (functional name: ``map``).\n    The function can be any regular Python function or partial object.
API_2: EndOnDiskCacheHolder(datapipe, mode='wb', filepath_fn=None, *, same_filepath_fn=False, skip_read=False):\n    Indicates when the result of prior DataPipe will be saved local files specified\n    by ``filepath_fn`` (functional name: ``end_caching``). Moreover, the result of source DataPipe\n    is required to be a tuple of metadata and data, or a tuple of metadata and file handle.
API_3: cycle(*args, **kwds):\n    Cycles the specified input in perpetuity by default, or for the specified number of times (functional name: ``cycle``).

Judgment: A

By using above documents, you can generate the code snippet like 'datapipe.cycle(6)'.
As documents support clear, accurate, and engaging answers, query is answerable so your output should be A.

### Demo 2
Question: Read the URL using the HTTP protocol and process the csv file.

Candidate Documents
API_1: IoPathFileLister(*args, **kwds):\n    Lists the contents of the directory at the provided ``root`` pathname or URL, and yields the full pathname or URL for each file within the directory.
API_2: OnlineReader(*args, **kwds):\n    Takes file URLs (can be HTTP URLs pointing to files or URLs to GDrive files), and yields tuples of file URL and IO stream.
API_3: HttpReader(source_datapipe: IterDataPipe[str], timeout: Optional[float] = None):\n    Takes file URLs (HTTP URLs pointing to files), and yields tuples of file URL and IO stream.

Judgment: P

You can read the URL with API_3 HttpReader, but you cannot process the csv file with above documents.
You can partially answer to the query, so your output should be P.

### Demo 3
Question: Clone the source datapipe two times

Candidate Documents
API_1: unbatch(datapipe: IterDataPipe, unbatch_level: int = 1):\n    Undoes batching of data (functional name: ``unbatch``). In other words, it flattens the data up to the specified level within a batched DataPipe.
API_2: IterToMapConverter(*args, **kwds):\n    Lazily load data from ``IterDataPipe`` to construct a ``MapDataPipe`` with the key-value pair generated by ``key_value_fn`` (functional name: ``to_map_datapipe``).
API_3: cycle(*args, **kwds):\n    Cycles the specified input in perpetuity by default, or for the specified number of times (functional name: ``cycle``).

Judgment: U

You may need information of methods like 'clone()' to resolve the question.
But there's no relevant information about the methods. You are unanswerble to the query, so your output should be U.

### Demo 4
Question: Join the three data pipes and obtain the enumerated datapipe.

Candidate Documents
API_1: collate(datapipe: IterDataPipe, collate_fn: Callable = <function default_collate>):\n    Collates samples from DataPipe to Tensor(s) by a custom collate function (functional name: ``collate``)
API_2: enumerate(*args, **kwds):\n    Adds an index to an existing DataPipe through enumeration, with\n    the index starting from 0 by default (functional name: ``enumerate``).
API_3: concat(*args, **kwds):\n    Concatenate multiple Map DataPipes (functional name: ``concat``).\n    The new index of is the cumulative sum of source DataPipes.

Judgment: A

By using above documents, you can generate the code snippet like 'dp_source_1.concat(dp_source_2).enumerate()'.
As documents support clear, accurate, and engaging answers, query is answerable so your output should be A.

### Demo 5
Question: Concatenate two datapipes and add corresponding indices with the name `Ids`.

Candidate Documents
API_1: MapDataPipe(*args, **kwds):\n    Map-style DataPipe.\n\n    All datasets that represent a map from keys to data samples should subclass this.
API_2: collate(datapipe: IterDataPipe, collate_fn: Callable = <function default_collate>):\n    Collates samples from DataPipe to Tensor(s) by a custom collate function (functional name: ``collate``).
API_3: concat(*args, **kwds):\n    Concatenate multiple Map DataPipes (functional name: ``concat``).\n    The new index of is the cumulative sum of source DataPipes.

Judgment: P

You can concatenate datapipes using API_2 collate, but you cannot add indices with above documents.
You can partially answer to the query, so your output should be P.

### Demo 6
Question: Putting two IterDataPipes together based on their key.

Candidate Documents
API_1: SampleMultiplexer(*args, **kwds):\n    Takes a `Dict` of (IterDataPipe, Weight), and yields items by sampling from these DataPipes with respect to their weights.
API_2: IterDataPipe(*args, **kwds):\n    Iterable-style DataPipe.\n\n    All DataPipes that represent an iterable of data samples should subclass this.
API_3: mux(*datapipes):\n    Yields one element at a time from each of the input Iterable DataPipes (functional name: ``mux``).

Judgment: U

You may need information of methods like 'zip_with_iter()' to resolve the question.
But there's no relevant information about the methods. You are unanswerble to the query, so your output should be U.

Question: {{query}}

Candidate Documents
{{apis}}

Judgment:"""

BEATNUM_ONE_SHOT_PROMPT = f"""## Profile
- Role: AnswerabilityVerificationGPT, evaluating whether provided documents can support answering the user's questions effectively.

### Input
- Question: User's specific question(s).
- Candidate Documents: Documents potentially useful for answering the questions.

### Skill
1. Analyzing questions to understand required information.
2. Assessing documents to determine their ability to support clear and accurate answers.

### Output
- Judgment: A, P, or U based on document suitability.

### Output Format
Judgment: A, P, or U

## Rules
1. Maintain character.
2. Provide final verdict only as "A", "P", or "U".
3. Do not evaluate demonstration, only assess question(s) and documents.
4. Adhere strictly to output format.
5. 'A' stands for [Answerable], 'P' stands for [Partially answerable] and 'U' stands for [Unanswerable]

## Judgment Criteria
1. Document length should not influence evaluation.
2. Strive for objectivity.
3. "A"(Answerable) if documents support clear, accurate, and engaging answers, "P"(Partially answerable) if some aspects can be answered, "U"(Unanswerable) 
if no relevant information.

## Workflow
1. Understand user questions.
2. Evaluate documents for answer support.
3. Provide final judgment.

## Reminder
Always remember the role settings.

## Demonstration
### Demo 1
Question: create a beatnum numset composed of a list [[8, 7, 2], [5, 6, 1], [8, 2, 6]]

Candidate Documents
API_1: numset(obj, itemsize=None, copy=True, unicode=None, order=None):\n    Create a `charnumset`.
API_2: difference(a, n=1, axis=-1, prepend=<no value>, apd=<no value>):\n    Calculate the n-th discrete difference along the given axis.
API_3: change_shape_to(a, newshape, order='C'):\n    Gives a new shape to an numset without changing its data.

Judgment: A

By using above documents, you can generate the code snippet like 'bn.numset([[8, 7, 2], [5, 6, 1], [8, 2, 6]])'.
As documents support clear, accurate, and engaging answers, query is answerable so your output should be A.

### Demo 2
Question: Find nearest value in beatnum numset return the result

Candidate Documents
API_1: uniq(ar, return_index=False, return_inverse=False, return_counts=False, axis=None):\n    Find the uniq elements of an numset.
API_2: arr_range(*args, **params):arr_range([start,] stop[, step,], dtype=None, *, like=None)\n\n    Return evenly spaced values within a given interval.
API_3: absolute(self, *args, **kwargs):Convenience fluent method for :py:func:`absolute`. The arguments are the same as for :py:func:`absolute`, with this numset as data.

Judgment: P

You can get the difference between each values with API_3 absolute, but you cannot get the nearest values as there's no information about argmin.
You can partially answer to the query, so your output should be P.

### Demo 3
Question: Convert beatnum numset type and values from Float64 to Float32

Candidate Documents
API_1: difference(a, n=1, axis=-1, prepend=<no value>, apd=<no value>):\n    Calculate the n-th discrete differenceerence along the given axis.
API_2: average(a, axis=None, dtype=None, out=None, keepdims=False):\n    Compute the arithmetic average along the specified axis. Returns the average of the numset elements.
API_3: total_count(a, axis=None, dtype=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>):\n    Sum of numset elements over a given axis.

Judgment: U

You may need information of methods like 'convert_type()' to resolve the question.
But there's no relevant information about the methods. You are unanswerble to the query, so your output should be U.

Question: {{query}}

Candidate Documents
{{apis}}

Judgment:"""

BEATNUM_TWO_SHOT_PROMPT = f"""## Profile
- Role: AnswerabilityVerificationGPT, evaluating whether provided documents can support answering the user's questions effectively.

### Input
- Question: User's specific question(s).
- Candidate Documents: Documents potentially useful for answering the questions.

### Skill
1. Analyzing questions to understand required information.
2. Assessing documents to determine their ability to support clear and accurate answers.

### Output
- Judgment: A, P, or U based on document suitability.

### Output Format
Judgment: A, P, or U

## Rules
1. Maintain character.
2. Provide final verdict only as "A", "P", or "U".
3. Do not evaluate demonstration, only assess question(s) and documents.
4. Adhere strictly to output format.
5. 'A' stands for [Answerable], 'P' stands for [Partially answerable] and 'U' stands for [Unanswerable]

## Judgment Criteria
1. Document length should not influence evaluation.
2. Strive for objectivity.
3. "A"(Answerable) if documents support clear, accurate, and engaging answers, "P"(Partially answerable) if some aspects can be answered, "U"(Unanswerable) 
if no relevant information.

## Workflow
1. Understand user questions.
2. Evaluate documents for answer support.
3. Provide final judgment.

## Reminder
Always remember the role settings.

## Demonstration
## Demo 1
Question: create a beatnum numset composed of a list [[8, 7, 2], [5, 6, 1], [8, 2, 6]]

Candidate Documents
API_1: numset(obj, itemsize=None, copy=True, unicode=None, order=None):\n    Create a `charnumset`.
API_2: difference(a, n=1, axis=-1, prepend=<no value>, apd=<no value>):\n    Calculate the n-th discrete difference along the given axis.
API_3: change_shape_to(a, newshape, order='C'):\n    Gives a new shape to an numset without changing its data.

Judgment: A

By using above documents, you can generate the code snippet like 'bn.numset([[8, 7, 2], [5, 6, 1], [8, 2, 6]])'.
As documents support clear, accurate, and engaging answers, query is answerable so your output should be A.

## Demo 2
Question: Find nearest value in beatnum numset return the result

Candidate Documents
API_1: uniq(ar, return_index=False, return_inverse=False, return_counts=False, axis=None):\n    Find the uniq elements of an numset.
API_2: arr_range(*args, **params):arr_range([start,] stop[, step,], dtype=None, *, like=None)\n\n    Return evenly spaced values within a given interval.
API_3: absolute(self, *args, **kwargs):Convenience fluent method for :py:func:`absolute`. The arguments are the same as for :py:func:`absolute`, with this numset as data.

Judgment: P

You can get the difference between each values with API_3 absolute, but you cannot get the nearest values as there's no information about argmin.
You can partially answer to the query, so your output should be P.

## Demo 3
Question: Convert beatnum numset type and values from Float64 to Float32

Candidate Documents
API_1: difference(a, n=1, axis=-1, prepend=<no value>, apd=<no value>):\n    Calculate the n-th discrete differenceerence along the given axis.
API_2: average(a, axis=None, dtype=None, out=None, keepdims=False):\n    Compute the arithmetic average along the specified axis. Returns the average of the numset elements.
API_3: total_count(a, axis=None, dtype=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>):\n    Sum of numset elements over a given axis.

Judgment: U

You may need information of methods like 'convert_type()' to resolve the question.
But there's no relevant information about the methods. You are unanswerble to the query, so your output should be U.

## Demo 4
Question: I'd like to calculate element-wise average between a, b and c.

Candidate Documents
API_1: arr_range(*args, **params):arr_range([start,] stop[, step,], dtype=None, *, like=None)\n\n    Return evenly spaced values within a given interval.
API_2: logic_and_element_wise(a, b, *args, **kwargs):logic_and_element_wise(x1, x2, /, out=None, *, filter_condition=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nCompute the truth value of x1 AND x2 element-wise.
API_3: average(a, axis=None, dtype=None, out=None, keepdims=False):\n    Compute the arithmetic average along the specified axis.\n    Returns the average of the numset elements.

Judgment: A

By using above documents, you can generate the code snippet like 'bn.average([a, b, c], axis=0)'.
As documents support clear, accurate, and engaging answers, query is answerable so your output should be A.

## Demo 5
Question: Can you transpose and flatten the numsets?

Candidate Documents
API_1: numset(obj, itemsize=None, copy=True, unicode=None, order=None):\n    Create a `charnumset`.
API_2: perform_partition(a, kth, axis=-1, kind='introselect', order=None):\n    Perform an indirect partition along the given axis using the algorithm specified by the `kind` keyword.
API_3: change_shape_to(a, newshape, order='C'):\n    Gives a new shape to an numset without changing its data.

Judgment: P

You can flatten the numset using API_3 change_shape_to(), but you cannot transpose it with above documents.
You can partially answer to the query, so your output should be P.

## Demo 6
Question: Connect a BeatNum numset to another BeatNum numset

Candidate Documents
API_1: average(a, axis=None, dtype=None, out=None, keepdims=False):\n    Compute the arithmetic average along the specified axis.
API_2: standard_op(self, axis=None, dtype=None, out=None, ddof=0):\n        Return the standard deviation of the numset elements along the given axis.
API_3: create_ones(shape, dtype=None, order='C', *, like=None):\n    Return a new numset of given shape and type, masked_fill with create_ones.

Judgment: U

You may need information of methods like 'concat()' to resolve the question.
But there's no relevant information about the methods. You are unanswerble to the query, so your output should be U.

Question: {{query}}

Candidate Documents
{{apis}}

Judgment:"""

MONKEY_ONE_SHOT_PROMPT = f"""## Profile
- Role: AnswerabilityVerificationGPT, evaluating whether provided documents can support answering the user's questions effectively.

### Input
- Question: User's specific question(s).
- Candidate Documents: Documents potentially useful for answering the questions.

### Skill
1. Analyzing questions to understand required information.
2. Assessing documents to determine their ability to support clear and accurate answers.

### Output
- Judgment: A, P, or U based on document suitability.

### Output Format
Judgment: A, P, or U

## Rules
1. Maintain character.
2. Provide final verdict only as "A", "P", or "U".
3. Do not evaluate demonstration, only assess question(s) and documents.
4. Adhere strictly to output format.
5. 'A' stands for [Answerable], 'P' stands for [Partially answerable] and 'U' stands for [Unanswerable]

## Judgment Criteria
1. Document length should not influence evaluation.
2. Strive for objectivity.
3. "A"(Answerable) if documents support clear, accurate, and engaging answers, "P"(Partially answerable) if some aspects can be answered, "U"(Unanswerable) 
if no relevant information.

## Workflow
1. Understand user questions.
2. Evaluate documents for answer support.
3. Provide final judgment.

## Reminder
Always remember the role settings.

## Demonstration
### Demo 1
Question: deleting a column from a Monkey KnowledgeFrame return the changged knowledgeframe

Candidate Documents
API_1: choose_dtypes(self, include=None, exclude=None) -> 'KnowledgeFrame':\n        Return a subset of the KnowledgeFrame's columns based on the column dtypes.
API_2: sip(self, labels, errors: 'str_t' = 'raise') -> 'Index':\n        Make new Index with passed list of labels deleted.
API_3: reseting_index(self, level: 'Hashable | Sequence[Hashable] | None' = None, sip: 'bool' = False, inplace: 'bool' = False, col_level: 'Hashable' = 0, col_fill: 'Hashable' = '') -> 'KnowledgeFrame | None':\n        Reset the index, or a level of it.

Judgment: A

By using above documents, you can generate the code snippet like 'kf.sip(column_name, axis=1)'.
As documents support clear, accurate, and engaging answers, query is answerable so your output should be A.

### Demo 2
Question: Return the knowledgeframe with the rows with one or more NaN values

Candidate Documents
API_1: division(self, other, axis='columns', level=None, fill_value=None):\nGet Floating divisionision of knowledgeframe and other, element-wise (binary operator `truedivision`).
API_2: whatever(self, *args, **kwargs):\n        Return whether whatever element is Truthy.
API_3: reseting_index(self, level: 'Hashable | Sequence[Hashable] | None' = None, sip: 'bool' = False, inplace: 'bool' = False, col_level: 'Hashable' = 0, col_fill: 'Hashable' = '') -> 'KnowledgeFrame | None':\n        Reset the index, or a level of it.

Judgment: P

You can check the condition of each rows with API_2 whatever, but there's no information about checking whether the value is NaN or not.
You can partially answer to the query, so your output should be P.

### Demo 3
Question: I want to make all column headers in my monkey data frame lower case

Candidate Documents
API_1: KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None):\n    Two-dimensional, size-mutable, potentitotal_ally heterogeneous tabular data.\n\n    Data structure also contains labeled axes (rows and columns).
API_2: concating(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, clone: 'bool' = True) -> 'FrameOrCollectionsUnion': Concatenate monkey objects along a particular axis with optional set logic\n    along the other axes.
API_3: choose_dtypes(self, include=None, exclude=None) -> 'KnowledgeFrame':\n        Return a subset of the KnowledgeFrame's columns based on the column dtypes.

Judgment: U

You may need information of methods like 'map()' to resolve the question.
But there's no relevant information about the methods. You are unanswerble to the query, so your output should be U.

Question: {{query}}

Candidate Documents
{{apis}}

Judgment:"""

MONKEY_TWO_SHOT_PROMPT = f"""## Profile
- Role: AnswerabilityVerificationGPT, evaluating whether provided documents can support answering the user's questions effectively.

### Input
- Question: User's specific question(s).
- Candidate Documents: Documents potentially useful for answering the questions.

### Skill
1. Analyzing questions to understand required information.
2. Assessing documents to determine their ability to support clear and accurate answers.

### Output
- Judgment: A, P, or U based on document suitability.

### Output Format
Judgment: A, P, or U

## Rules
1. Maintain character.
2. Provide final verdict only as "A", "P", or "U".
3. Do not evaluate demonstration, only assess question(s) and documents.
4. Adhere strictly to output format.
5. 'A' stands for [Answerable], 'P' stands for [Partially answerable] and 'U' stands for [Unanswerable]

## Judgment Criteria
1. Document length should not influence evaluation.
2. Strive for objectivity.
3. "A"(Answerable) if documents support clear, accurate, and engaging answers, "P"(Partially answerable) if some aspects can be answered, "U"(Unanswerable) 
if no relevant information.

## Workflow
1. Understand user questions.
2. Evaluate documents for answer support.
3. Provide final judgment.

## Reminder
Always remember the role settings.

## Demonstration
### Demo 1
Question: deleting a column from a Monkey KnowledgeFrame return the changged knowledgeframe

Candidate Documents
API_1: choose_dtypes(self, include=None, exclude=None) -> 'KnowledgeFrame':\n        Return a subset of the KnowledgeFrame's columns based on the column dtypes.
API_2: sip(self, labels, errors: 'str_t' = 'raise') -> 'Index':\n        Make new Index with passed list of labels deleted.
API_3: reseting_index(self, level: 'Hashable | Sequence[Hashable] | None' = None, sip: 'bool' = False, inplace: 'bool' = False, col_level: 'Hashable' = 0, col_fill: 'Hashable' = '') -> 'KnowledgeFrame | None':\n        Reset the index, or a level of it.

Judgment: A

By using above documents, you can generate the code snippet like 'kf.sip(column_name, axis=1)'.
As documents support clear, accurate, and engaging answers, query is answerable so your output should be A.

### Demo 2
Question: Return the knowledgeframe with the rows with one or more NaN values

Candidate Documents
API_1: division(self, other, axis='columns', level=None, fill_value=None):\nGet Floating divisionision of knowledgeframe and other, element-wise (binary operator `truedivision`).
API_2: whatever(self, *args, **kwargs):\n        Return whether whatever element is Truthy.
API_3: reseting_index(self, level: 'Hashable | Sequence[Hashable] | None' = None, sip: 'bool' = False, inplace: 'bool' = False, col_level: 'Hashable' = 0, col_fill: 'Hashable' = '') -> 'KnowledgeFrame | None':\n        Reset the index, or a level of it.

Judgment: P

You can check the condition of each rows with API_2 whatever, but there's no information about checking whether the value is NaN or not.
You can partially answer to the query, so your output should be P.

### Demo 3
Question: I want to make all column headers in my monkey data frame lower case

Candidate Documents
API_1: KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None):\n    Two-dimensional, size-mutable, potentitotal_ally heterogeneous tabular data.\n\n    Data structure also contains labeled axes (rows and columns).
API_2: concating(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, clone: 'bool' = True) -> 'FrameOrCollectionsUnion': Concatenate monkey objects along a particular axis with optional set logic\n    along the other axes.
API_3: choose_dtypes(self, include=None, exclude=None) -> 'KnowledgeFrame':\n        Return a subset of the KnowledgeFrame's columns based on the column dtypes.

Judgment: U

You may need information of methods like 'map()' to resolve the question.
But there's no relevant information about the methods. You are unanswerble to the query, so your output should be U.

### Demo 4
Question: How do I combine two knowledgeframes with ignore index? Return the concated knowledgeframe.

Candidate Documents
API_1: division(self, other, axis='columns', level=None, fill_value=None):\nGet Floating divisionision of knowledgeframe and other, element-wise (binary operator `truedivision`)
API_2: adding(self, other: 'Index | Sequence[Index]') -> 'Index':\n        Append a collection of Index options together.
API_3: concating(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, clone: 'bool' = True) -> 'FrameOrCollectionsUnion':\n    Concatenate monkey objects along a particular axis with optional set logic along the other axes.

Judgment: A

By using above documents, you can generate the code snippet like 'kf1.adding(kf2, ignore_index=True)'.
As documents support clear, accurate, and engaging answers, query is answerable so your output should be A.

### Demo 5
Question: Add a new column named 'Fruit Total' that sums the values of the other columns Note that igonring the NaN values

Candidate Documents
API_1: unioner(self, right: 'FrameOrCollectionsUnion', how: 'str' = 'inner', on: 'IndexLabel | None' = None, left_on: 'IndexLabel | None' = None, right_on: 'IndexLabel | None' = None, left_index: 'bool' = False, right_index: 'bool' = False, sort: 'bool' = False, suffixes: 'Suffixes' = ('_x', '_y'), clone: 'bool' = True, indicator: 'bool' = False, validate: 'str | None' = None) -> 'KnowledgeFrame':\nMerge KnowledgeFrame or named Collections objects with a database-style join.
API_2: total_sum(self, axis=None, skipna=None, level=None, numeric_only=None, getting_min_count=0, **kwargs):\nReturn the total_sum of the values over the requested axis.\n\nThis is equivalengtht to the method ``numpy.total_sum``.
API_3: allocate(self, **kwargs) -> 'KnowledgeFrame':\n        Assign new columns to a KnowledgeFrame.\n\n        Returns a new object with total_all original columns in addition to new ones.

Judgment: P

You can get the sum of the value with API_2 total_sum(), but there's no information about mapping or iteratign the kf.
You can partially answer to the query, so your output should be P.

### Demo 6
Question: How would I rename the only one column header? return the changed knowledgeframe

Candidate Documents
API_1: convert_string(self, buf: 'FilePathOrBuffer[str] | None' = None, columns: 'Sequence[str] | None' = None, col_space: 'int | None' = None, header_numer: 'bool | Sequence[str]' = True, index: 'bool' = True, na_rep: 'str' = 'NaN', formatingters: 'fmt.FormattersType | None' = None, float_formating: 'fmt.FloatFormatType | None' = None, sparsify: 'bool | None' = None, index_names: 'bool' = True, justify: 'str | None' = None, getting_max_rows: 'int | None' = None, getting_min_rows: 'int | None' = None, getting_max_cols: 'int | None' = None, show_dimensions: 'bool' = False, decimal: 'str' = '.', line_width: 'int | None' = None, getting_max_colwidth: 'int | None' = None, encoding: 'str | None' = None) -> 'str | None':\n        Render a KnowledgeFrame to a console-friendly tabular output.
API_2: division(self, other, axis='columns', level=None, fill_value=None):\nGet Floating divisionision of knowledgeframe and other, element-wise (binary operator `truedivision`).\n\nEquivalengtht to ``knowledgeframe / other``, but with support to substitute a fill_value\nfor missing data in one of the inputs.
API_3: reseting_index(self, level: 'Hashable | Sequence[Hashable] | None' = None, sip: 'bool' = False, inplace: 'bool' = False, col_level: 'Hashable' = 0, col_fill: 'Hashable' = '') -> 'KnowledgeFrame | None':\n        Reset the index, or a level of it.\n\n        Reset the index of the KnowledgeFrame, and use the default one instead.

Judgment: U

You may need information of methods like 'renaming()' to resolve the question.
But there's no relevant information about the methods. You are unanswerble to the query, so your output should be U.

Question: {{query}}

Candidate Documents
{{apis}}

Judgment:"""

PYNP_ONE_SHOT_PROMPT = f"""## Profile
- Role: AnswerabilityVerificationGPT, evaluating whether provided documents can support answering the user's questions effectively.

### Input
- Question: User's specific question(s).
- Candidate Documents: Documents potentially useful for answering the questions.

### Skill
1. Analyzing questions to understand required information.
2. Assessing documents to determine their ability to support clear and accurate answers.

### Output
- Judgment: A, P, or U based on document suitability.

### Output Format
Judgment: A, P, or U

## Rules
1. Maintain character.
2. Provide final verdict only as "A", "P", or "U".
3. Do not evaluate demonstration, only assess question(s) and documents.
4. Adhere strictly to output format.
5. 'A' stands for [Answerable], 'P' stands for [Partially answerable] and 'U' stands for [Unanswerable]

## Judgment Criteria
1. Document length should not influence evaluation.
2. Strive for objectivity.
3. "A"(Answerable) if documents support clear, accurate, and engaging answers, "P"(Partially answerable) if some aspects can be answered, "U"(Unanswerable) 
if no relevant information.

## Workflow
1. Understand user questions.
2. Evaluate documents for answer support.
3. Provide final judgment.

## Reminder
Always remember the role settings.

## Demonstration
### Demo 1
Question: How can I compress a custom ONNX model using singular value decomposition?

Candidate Documents
API_1: Converter.convert_model(input_model_path: str, output_dir: str, target_framework: Union[str, Framework], target_device_name: Union[str, DeviceName], target_data_type: Union[str, DataType] = DataType.FP16, target_software_version: Optional[Union[str, SoftwareVersion]] = None, input_shape: Optional[InputShape] = None, dataset_path: Optional[str] = None, wait_until_done: bool = True): Convert a model to the specified framework. Returns model conversion task dictionary.
API_2: Compressor.recommendation_compression(compression_method: CompressionMethod, recommendation_method: RecommendationMethod, recommendation_ratio: float, input_model_path: str, output_dir: str, input_shapes: List[Dict[str, int]], framework: Framework = Framework.PYTORCH, options: Options = Options(), dataset_path: Optional[str] = None): Compress a recommendation-based model using the given compression and recommendation methods. Returns source model and compressed model information.
API_3: Compressor.automatic_compression(input_model_path: str, output_dir: str, input_shapes: List[Dict[str, int]], framework: Framework = Framework.PYTORCH, compression_ratio: float = 0.5): Compress a model automatically based on the given compression ratio.

Judgment: A

By using above documents, you can compress the model using API_2 Compressor.recommendation_compression().
As documents support clear, accurate, and engaging answers, query is answerable so your output should be A.

### Demo 2
Question: Show me an example of training a ResNet model for image classification.

Candidate Documents
API_1: Trainer.set_model_config(model_name: str, img_size: int, use_pretrained: bool = True, load_head: bool = False, path: Optional[str] = None, fx_model_path: Optional[str] = None, optimizer_path: Optional[str] = None): Set the model configuration for the Trainer.
API_2: Trainer.set_logging_config(project_id: Optional[str] = None, output_dir: str = './outputs', tensorboard: bool = True, csv: bool = False, image: bool = True, stdout: bool = True, save_optimizer_state: bool = True, validation_epoch: int = 10, save_checkpoint_epoch: Optional[int] = None): Set the logging configuration.
API_3: Trainer.set_training_config(optimizer, scheduler, epochs: int = 3, batch_size: int = 8): Set the training configuration.

Judgment: P

You can set ResNet model using API_1 Trainer.set_model_config(), but you cannot select the task with above documents.
You can partially answer to the query, so your output should be P.

### Demo 3
Question: How do I set random seed for environmental setup?

Candidate Documents
API_1: Trainer.set_logging_config(project_id: Optional[str] = None, output_dir: str = './outputs', tensorboard: bool = True, csv: bool = False, image: bool = True, stdout: bool = True, save_optimizer_state: bool = True, validation_epoch: int = 10, save_checkpoint_epoch: Optional[int] = None): Set the logging configuration.
API_2: Trainer.train(gpus: str, project_name: str): Train the model with the specified configuration. Returns a dictionary containing information about the training.\n\n\nParameters\n----------\ngpus (str): GPU ids to use, separated by commas.\nproject_name (str): Project name to save the experiment.
API_3: Trainer.set_training_config(optimizer, scheduler, epochs: int = 3, batch_size: int = 8): Set the training configuration.

Judgment: U

You may need information of methods like 'set_environment_config()' to resolve the question.
But there's no relevant information about the methods. You are unanswerble to the query, so your output should be U.

Question: {{query}}

Candidate Documents
{{apis}}

Judgment:"""

PYNP_TWO_SHOT_PROMPT = f"""## Profile
- Role: AnswerabilityVerificationGPT, evaluating whether provided documents can support answering the user's questions effectively.

### Input
- Question: User's specific question(s).
- Candidate Documents: Documents potentially useful for answering the questions.

### Skill
1. Analyzing questions to understand required information.
2. Assessing documents to determine their ability to support clear and accurate answers.

### Output
- Judgment: A, P, or U based on document suitability.

### Output Format
Judgment: A, P, or U

## Rules
1. Maintain character.
2. Provide final verdict only as "A", "P", or "U".
3. Do not evaluate demonstration, only assess question(s) and documents.
4. Adhere strictly to output format.
5. 'A' stands for [Answerable], 'P' stands for [Partially answerable] and 'U' stands for [Unanswerable]

## Judgment Criteria
1. Document length should not influence evaluation.
2. Strive for objectivity.
3. "A"(Answerable) if documents support clear, accurate, and engaging answers, "P"(Partially answerable) if some aspects can be answered, "U"(Unanswerable) 
if no relevant information.

## Workflow
1. Understand user questions.
2. Evaluate documents for answer support.
3. Provide final judgment.

## Reminder
Always remember the role settings.

## Demonstration
### Demo 1
Question: How can I compress a custom ONNX model using singular value decomposition?

Candidate Documents
API_1: Converter.convert_model(input_model_path: str, output_dir: str, target_framework: Union[str, Framework], target_device_name: Union[str, DeviceName], target_data_type: Union[str, DataType] = DataType.FP16, target_software_version: Optional[Union[str, SoftwareVersion]] = None, input_shape: Optional[InputShape] = None, dataset_path: Optional[str] = None, wait_until_done: bool = True): Convert a model to the specified framework. Returns model conversion task dictionary.
API_2: Compressor.recommendation_compression(compression_method: CompressionMethod, recommendation_method: RecommendationMethod, recommendation_ratio: float, input_model_path: str, output_dir: str, input_shapes: List[Dict[str, int]], framework: Framework = Framework.PYTORCH, options: Options = Options(), dataset_path: Optional[str] = None): Compress a recommendation-based model using the given compression and recommendation methods. Returns source model and compressed model information.
API_3: Compressor.automatic_compression(input_model_path: str, output_dir: str, input_shapes: List[Dict[str, int]], framework: Framework = Framework.PYTORCH, compression_ratio: float = 0.5): Compress a model automatically based on the given compression ratio.

Judgment: A

By using above documents, you can compress the model using API_2 Compressor.recommendation_compression().
As documents support clear, accurate, and engaging answers, query is answerable so your output should be A.

### Demo 2
Question: Show me an example of training a ResNet model for image classification.

Candidate Documents
API_1: Trainer.set_model_config(model_name: str, img_size: int, use_pretrained: bool = True, load_head: bool = False, path: Optional[str] = None, fx_model_path: Optional[str] = None, optimizer_path: Optional[str] = None): Set the model configuration for the Trainer.
API_2: Trainer.set_logging_config(project_id: Optional[str] = None, output_dir: str = './outputs', tensorboard: bool = True, csv: bool = False, image: bool = True, stdout: bool = True, save_optimizer_state: bool = True, validation_epoch: int = 10, save_checkpoint_epoch: Optional[int] = None): Set the logging configuration.
API_3: Trainer.set_training_config(optimizer, scheduler, epochs: int = 3, batch_size: int = 8): Set the training configuration.

Judgment: P

You can set ResNet model using API_1 Trainer.set_model_config(), but you cannot select the task with above documents.
You can partially answer to the query, so your output should be P.

### Demo 3
Question: How do I set random seed for environmental setup?

Candidate Documents
API_1: Trainer.set_logging_config(project_id: Optional[str] = None, output_dir: str = './outputs', tensorboard: bool = True, csv: bool = False, image: bool = True, stdout: bool = True, save_optimizer_state: bool = True, validation_epoch: int = 10, save_checkpoint_epoch: Optional[int] = None): Set the logging configuration.
API_2: Trainer.train(gpus: str, project_name: str): Train the model with the specified configuration. Returns a dictionary containing information about the training.\n\n\nParameters\n----------\ngpus (str): GPU ids to use, separated by commas.\nproject_name (str): Project name to save the experiment.
API_3: Trainer.set_training_config(optimizer, scheduler, epochs: int = 3, batch_size: int = 8): Set the training configuration.

Judgment: U

You may need information of methods like 'set_environment_config()' to resolve the question.
But there's no relevant information about the methods. You are unanswerble to the query, so your output should be U.

### Demo 4
Question: What method do I use to set an optimizer and a scheduler? Please provide an example.

Candidate Documents
API_1: Compressor.automatic_compression(input_model_path: str, output_dir: str, input_shapes: List[Dict[str, int]], framework: Framework = Framework.PYTORCH, compression_ratio: float = 0.5): Compress a model automatically based on the given compression ratio.
API_2: Trainer.set_training_config(optimizer, scheduler, epochs: int = 3, batch_size: int = 8): Set the training configuration.
API_3: Trainer.set_logging_config(project_id: Optional[str] = None, output_dir: str = './outputs', tensorboard: bool = True, csv: bool = False, image: bool = True, stdout: bool = True, save_optimizer_state: bool = True, validation_epoch: int = 10, save_checkpoint_epoch: Optional[int] = None): Set the logging configuration.

Judgment: A

By using above documents, you can set an optimizer and a scheduler using API_2: Trainer.set_training_config().
As documents support clear, accurate, and engaging answers, query is answerable so your output should be A.

### Demo 5
Question: What are the key differences between the automatic_compression and recommendation_compression methods?

Candidate Documents
API_1: Compressor.get_compression(compression_id: str): Get information about a compression. Returns the information about the compression.
API_2: Compressor.recommendation_compression(compression_method: CompressionMethod, recommendation_method: RecommendationMethod, recommendation_ratio: float, input_model_path: str, output_dir: str, input_shapes: List[Dict[str, int]], framework: Framework = Framework.PYTORCH, options: Options = Options(), dataset_path: Optional[str] = None): Compress a recommendation-based model using the given compression and recommendation methods.
API_3: Compressor.compress_model(compression: CompressionInfo, output_dir: str, dataset_path: Optional[str] = None): Compress a model using the provided compression information. Returns source model and compressed model information.

Judgment: P

You would know about recommendation_compression, but you cannot know what is automatic_compression with above documents.
You can partially answer to the query, so your output should be P.

### Demo 6
Question: How do I configure the dataset?

Candidate Documents
API_1: Trainer.set_augmentation_config(train_transforms: Optional[List] = None, train_mix_transforms: Optional[List] = None, inference_transforms: Optional[List] = None): Set the augmentation configuration for training.
API_2: Trainer.set_model_config(model_name: str, img_size: int, use_pretrained: bool = True, load_head: bool = False, path: Optional[str] = None, fx_model_path: Optional[str] = None, optimizer_path: Optional[str] = None): Set the model configuration for the Trainer.
API_3: Trainer.set_logging_config(project_id: Optional[str] = None, output_dir: str = './outputs', tensorboard: bool = True, csv: bool = False, image: bool = True, stdout: bool = True, save_optimizer_state: bool = True, validation_epoch: int = 10, save_checkpoint_epoch: Optional[int] = None): Set the logging configuration.

Judgment: U

You may need information of methods like 'set_dataset_config()' to resolve the question.
But there's no relevant information about the methods. You are unanswerble to the query, so your output should be U.

Question: {{query}}

Candidate Documents
{{apis}}

Judgment:"""


SINGLE_TOKEN_BASELINE_PROMPT = f"""## Profile
- Role: AnswerabilityVerificationGPT, evaluating whether provided documents can support answering the user's questions effectively.

### Input
- Question: User's specific question(s).
- Candidate Documents: Documents potentially useful for answering the questions.

### Skill
1. Analyzing questions to understand required information.
2. Assessing documents to determine their ability to support clear and accurate answers.

### Output
- Judgment: A, P, or U based on document suitability.

### Output Format
Judgment: A, P, or U

## Rules
1. Maintain character.
2. Provide final verdict only as "A", "P", or "U".
3. Do not evaluate demonstration, only assess question(s) and documents.
4. Adhere strictly to output format.
5. 'A' stands for [Answerable], 'P' stands for [Partially answerable] and 'U' stands for [Unanswerable]

## Judgment Criteria
1. Document length should not influence evaluation.
2. Strive for objectivity.
3. "A"(Answerable) if documents support clear, accurate, and engaging answers, "P"(Partially answerable) if some aspects can be answered, "U"(Unanswerable) 
if no relevant information.

## Workflow
1. Understand user questions.
2. Evaluate documents for answer support.
3. Provide final judgment.

## Reminder
Always remember the role settings.

Question: {{query}}

Candidate Documents
{{apis}}

Judgment:"""

exception_query_list = {
    "one": [
        "How to augument the datapipe by repeating it six times.",
        "Clone the source datapipe two times",
        "Concatenate two datapipes and add corresponding indices with the name `Ids`.",
        "create a beatnum numset composed of a list [[8, 7, 2], [5, 6, 1], [8, 2, 6]]",
        "Convert beatnum numset type and values from Float64 to Float32",
        "Can you transpose and flatten the numsets?",
        "deleting a column from a Monkey KnowledgeFrame return the changged knowledgeframe",
        "I want to make all column headers in my monkey data frame lower case",
        "Add a new column named 'Fruit Total' that sums the values of the other columns Note that igonring the NaN values",
        "How can I compress a custom ONNX model using singular value decomposition?",
        "How do I set random seed for environmental setup?",
        "What are the key differences between the automatic_compression and recommendation_compression methods?",
    ],
    "two": [
        "How to augument the datapipe by repeating it six times.",
        "Read the URL using the HTTP protocol and process the csv file.",
        "Clone the source datapipe two times",
        "Join the three data pipes and obtain the enumerated datapipe.",
        "Concatenate two datapipes and add corresponding indices with the name `Ids`.",
        "Putting two IterDataPipes together based on their key.",
        "create a beatnum numset composed of a list [[8, 7, 2], [5, 6, 1], [8, 2, 6]]",
        "Find nearest value in beatnum numset return the result",
        "Convert beatnum numset type and values from Float64 to Float32",
        "I'd like to calculate element-wise average between a, b and c.",
        "Can you transpose and flatten the numsets?",
        "Connect a BeatNum numset to another BeatNum numset",
        "deleting a column from a Monkey KnowledgeFrame return the changged knowledgeframe",
        "Return the knowledgeframe with the rows with one or more NaN values",
        "I want to make all column headers in my monkey data frame lower case",
        "How do I combine two knowledgeframes with ignore index? Return the concated knowledgeframe.",
        "Add a new column named 'Fruit Total' that sums the values of the other columns Note that igonring the NaN values",
        "How would I rename the only one column header? return the changed knowledgeframe",
        "How can I compress a custom ONNX model using singular value decomposition?",
        "Show me an example of training a ResNet model for image classification.",
        "How do I set random seed for environmental setup?",
        "What method do I use to set an optimizer and a scheduler? Please provide an example.",
        "What are the key differences between the automatic_compression and recommendation_compression methods?",
        "How do I configure the dataset?",
    ],
}


prompt_mapping = {
    "TorchData": {"one": TORCHDATA_ONE_SHOT_PROMPT, "two": TORCHDATA_TWO_SHOT_PROMPT},
    "BeatNum": {"one": BEATNUM_ONE_SHOT_PROMPT, "two": BEATNUM_TWO_SHOT_PROMPT},
    "Monkey": {"one": MONKEY_ONE_SHOT_PROMPT, "two": MONKEY_TWO_SHOT_PROMPT},
    "NetsPresso": {"one": PYNP_ONE_SHOT_PROMPT, "two": PYNP_TWO_SHOT_PROMPT},
}
